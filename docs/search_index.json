[["index.html", "GEOG0114: Principles of Spatial Analysis About Moodle", " GEOG0114: Principles of Spatial Analysis Justin van Dijk 2021-09-08 About Welcome to Principles of Spatial Analysis, one of the four core modules for the MSc in Geographic and Social Data Science here at UCL Geography. This handbook contains practical material for four weeks of this year’s module. Moodle Moodle is the central point of your learning experience for GEOG0114 and contains everything you need to know about it. It’s where you’ll find links to all lecture content, reading materials, planned activities as well as key module and assessment information "],["spatial-operations.html", "1 Spatial operations 1.1 This week 1.2 Case study 1.3 Getting started 1.4 Libraries 1.5 Loading our datasets 1.6 Data Processing 1.7 Greenspace in London 1.8 Attributions", " 1 Spatial operations 1.1 This week Understanding spatial properties, relationships and how they are used within spatial operations are the building blocks to spatial data processing and analysis. This tutorial takes you through a simple approach to measuring greenspace access for schools in London, using geometric operations as the main methods for processing and analysing your data. You will construct a buffer dataset around our greenspace and determine whether nearby schools intersect with this buffer. We will first visualise our data as points to see if we can identify areas of high versus low access - and then aggregate the data to the ward level for potential further use within analysis with statistical data, such as census information. 1.2 Case study Recent research (Bijnens et al. 2020) has shown that children brought up in proximity to greenspace have a higher IQ and fewer behavioral problems, irrespective of socio-economic background. In our analysis today, we will look to understand whether there are geographical patterns to schools that have high versus low access of greenspace and where a lack of greenspace needs to be addressed in London. Below, we can see where schools are located in London and get a general understanding of their proximity to large greenspace just through a simple navigation of the map. In this practical we will try to quantify these visual patterns we may observe and find out which schools are within 400 metres of greenspace that is larger than 50,000 square meters. We then calculate for each ward the percentage of schools that have access to a large greenspace. 1.3 Getting started To enable the efficient, repeatable and reproducible functionality of our work, we will use R-Studio’s ability to create and host code as a script. Before we do anything therefore, we will need to create a new R script: File &gt; New File &gt; R Script Let’s go ahead and save our script now, so we know it’s stored in our system - and in the future, we only need to remind ourselves to complete a quick save (e.g. cmd / ctrl + s). Now we have our script created, we can create a new data folder within the same directory as where we saved our script then copy over our data into this folder. Call this folder data. Next, if you haven’t already, download this week’s practical data zipfile. Once downloaded, we want to move this zipfile from your Downloads and into this newly created data folder. You can do this either in your computer OS’s normal file management tool, e.g. finder in Mac OS, or you can do this using R-Studio within the Files pane. Lastly, we want to unzip the file to gain access to the data within our R environment. File download File Type Link London school and greenspace data shp Download 1.4 Libraries The first two tasks you will do everytime you create a new script is to first, point your computer to your working directory (so it knows where all your data is) and second, (pre-emptively) load many of the libraries you think you’ll be using in your analysis. Luckily, we can now use the latter to also do the former - saving us a little bit of time in our set-up and a lot of time in our script-writing later on - by using the here library. Install this library if you have not already done so! # libraries library(here) ## here() starts at /Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114 But this library alone will not be enough for our analysis today! At the moment, we know that we’ll need to load some spatial data - therefore we need to load a library capable of handling our datasets. For this module, and preferably within your R programming moving forward, we will focus on using the sf or Simple Features library that allows us to load, manipulate, process and export spatial data. Note Prior to the sf library, which was introduced to R-Studio in 2017, the sp library was the main spatial library used in R-Studio. As a result, you may see older code or scripts using the sp to handle spatial data. sf has replaced sp as the default spatial library as it works better with the tidyverse way of doing things. Ultimately, you can convert between the two library formats (and some other libraries we will use later on in the term still only work with sp) - but it is best practice to try to use the sf library in your code moving forward. In addition to the sf library, we want to add in the tidyverse library that will allow us to use the pipe function (%&gt;%), amongst other things, within our work and enable more efficient programming. We are likely going to need some additional libraries to help further manipulate or visualise our datasets as we move forward with our processing - we’ll add these in now, but explain them in a little more detail as we get to use them in our code. These libraries include: units and tmap. As a result, the top of our script should look something like: # libraries library(here) library(tidyverse) library(sf) library(tmap) library(units) 1.5 Loading our datasets For this analysis we have three different datasets available: schools in London, greenspace in London (split into two separate datasets), and wards (an administrative geography) in London. All three of our datasets are provided as shapefiles which will make working with the data relatively straight-forward (e.g. even for our point data, the schools, we do not need to convert them from a csv as we often find with this type of data). But we’ll need to do quite a few steps of processing to get our final dataset. Let’s go ahead and load our three variables - we will use the sf library st_read() command to load our datasets into variables for use within our code: # load london schools london_schools &lt;- st_read(here::here(&#39;data/schools&#39;, &#39;school_data_london_2016.shp&#39;)) ## Reading layer `school_data_london_2016&#39; from data source ## `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/data/schools/school_data_london_2016.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 3889 features and 44 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -63477.1 ymin: 6665527 xmax: 40792.11 ymax: 6751279 ## Projected CRS: WGS 84 / Pseudo-Mercator # load london wards london_wards &lt;- st_read(here::here(&#39;data/administrative_boundaries&#39;, &#39;london_wards.shp&#39;)) ## Reading layer `london_wards&#39; from data source ## `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/data/administrative_boundaries/london_wards.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 633 features and 6 fields ## Geometry type: POLYGON ## Dimension: XY ## Bounding box: xmin: 503568.2 ymin: 155850.8 xmax: 561957.5 ymax: 200933.9 ## Projected CRS: OSGB 1936 / British National Grid # load london greenspace TL_greenspace &lt;- st_read(here::here(&#39;data/greenspace&#39;, &#39;TL_GreenspaceSite.shp&#39;)) ## Reading layer `TL_GreenspaceSite&#39; from data source ## `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/data/greenspace/TL_GreenspaceSite.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 8614 features and 6 fields ## Geometry type: MULTIPOLYGON ## Dimension: XYZ ## Bounding box: xmin: 499139.7 ymin: 198946.3 xmax: 601149.4 ymax: 300165 ## z_range: zmin: 0 zmax: 0 ## Projected CRS: OSGB 1936 / British National Grid TQ_greenspace &lt;- st_read(here::here(&#39;data/greenspace&#39;, &#39;TQ_GreenspaceSite.shp&#39;)) ## Reading layer `TQ_GreenspaceSite&#39; from data source ## `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/data/greenspace/TQ_GreenspaceSite.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 19575 features and 6 fields ## Geometry type: MULTIPOLYGON ## Dimension: XYZ ## Bounding box: xmin: 499419.4 ymin: 99770.75 xmax: 600740.5 ymax: 201001.5 ## z_range: zmin: 0 zmax: 0 ## Projected CRS: OSGB 1936 / British National Grid To see what each variable looks like, you can type in plot(name_of_variable) into the R console. This is a quick command to understand both the spatial coverage and attributes of your data - as it will display the data by each of its attribute fields as a plot. 1.6 Data Processing Now we have our data loaded as variables, we’re ready to start processing! In spatial data processing, the question always is: where do I start first? And the easiest answer to that is: make sure all of your data is in the same Projected (or Coordinate) Reference System as each other. Checking - and changing projections - should always be the first step of any workflow as this will ensure you do not carry through any potential mistakes or errors that using the wrong system can cause. 1.6.1 Reprojecting When you loaded your datasets in the above step, you may have notice that in the console additional information about the dataset is printed - this includes the metadata on the dataset’s Coordinate Referene System! As a result, it is quite easy to simply scroll the terminal to check the CRS for each dataset - which as you’ll see, all the datasets bar the school are using EPSG 27700, whih is the code for British National Grid, whereas our schools dataset shows 3857, the code for WGS84. That means we need to start with our london_schools variable - as we know that this is the only dataset currently in the wrong projection (WGS84) instead of using British National Grid. To reproject our dataset, we can use a function within the sf library, known as st_transform(). It is very simple to use - you only need to provide the function with the dataset and the code for the new CRS you wish to use with the data. For now, we will simply store the result of this transformation as a new variable - but you could in the future, rewrite this code to use pipes to pipe this transformation when loading the dataset. # reproject london schools form WGS84 to BNG london_schools_prj &lt;- st_transform(london_schools, 27700) We can now double-check our new variable is in the correct CRS by typing the following into the console and checking the result: # check CRS st_crs(london_schools_prj) ## Coordinate Reference System: ## User input: EPSG:27700 ## wkt: ## PROJCRS[&quot;OSGB 1936 / British National Grid&quot;, ## BASEGEOGCRS[&quot;OSGB 1936&quot;, ## DATUM[&quot;OSGB 1936&quot;, ## ELLIPSOID[&quot;Airy 1830&quot;,6377563.396,299.3249646, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## ID[&quot;EPSG&quot;,4277]], ## CONVERSION[&quot;British National Grid&quot;, ## METHOD[&quot;Transverse Mercator&quot;, ## ID[&quot;EPSG&quot;,9807]], ## PARAMETER[&quot;Latitude of natural origin&quot;,49, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8801]], ## PARAMETER[&quot;Longitude of natural origin&quot;,-2, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433], ## ID[&quot;EPSG&quot;,8802]], ## PARAMETER[&quot;Scale factor at natural origin&quot;,0.9996012717, ## SCALEUNIT[&quot;unity&quot;,1], ## ID[&quot;EPSG&quot;,8805]], ## PARAMETER[&quot;False easting&quot;,400000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8806]], ## PARAMETER[&quot;False northing&quot;,-100000, ## LENGTHUNIT[&quot;metre&quot;,1], ## ID[&quot;EPSG&quot;,8807]]], ## CS[Cartesian,2], ## AXIS[&quot;(E)&quot;,east, ## ORDER[1], ## LENGTHUNIT[&quot;metre&quot;,1]], ## AXIS[&quot;(N)&quot;,north, ## ORDER[2], ## LENGTHUNIT[&quot;metre&quot;,1]], ## USAGE[ ## SCOPE[&quot;Engineering survey, topographic mapping.&quot;], ## AREA[&quot;United Kingdom (UK) - offshore to boundary of UKCS within 49°45&#39;N to 61°N and 9°W to 2°E; onshore Great Britain (England, Wales and Scotland). Isle of Man onshore.&quot;], ## BBOX[49.75,-9,61.01,2.01]], ## ID[&quot;EPSG&quot;,27700]] As you can see from the output above, our dataset has been reprojected into EPSG 27700 or British National Grid! The next step to process our london_schools_prj dataset is to reduce the schools to only our chosen London extent. As you may have seen from the map above, our schools cover an area larger than our usual London extent. We can even make a quick map of this to check this properly: # inspect tm_shape(london_wards) + tm_polygons() + tm_shape(london_schools_prj) + tm_dots() As we can see, we indeed have schools outside of our London wards - as a result, we want to remove those schools outside of this boundary. We will do this by first dissolving our ward file to create a more simplified shapefile for use as a “cookie-cutter”. 1.6.2 Dissolving To dissolve a polygon shapefile using R code, we will use the summarise() function that comes from the dplyr library (part of the tidyverse) and summarise our London wards dataset by summing its total area (supplied in the HECTARES attribute field/column) across all records. This will reduce our data frame to a single row, which will only contain one attribute - our total area of London, which we can then map/use as our clip (cookie-cutter) feature! # dissolve london_outline &lt;- london_wards %&gt;% summarise(area = sum(HECTARES)) # inspect tm_shape(london_outline) + tm_polygons() 1.6.3 Subsetting Now we have our London outline, we can go ahead and clip our schools dataset by our London outline. Whilst there is a clip function within the sf library, what we will do here is use a techinque known as spatial subsetting, which is more similar to selecting by location: we will subset our london schools dataset by filtering out those that are not within the London Outline. This approach in R is much quicker than using the clip function - although deciding which approach to use is not only a question of speed but also how each function will affect the filtered data. When using a clip function, the function acts exactly like a cookie-cutter and will trim off any data that overlaps with the boundaries used. Conversely, when using a subsetting approach, if a data point or polygon overlaps on the boundary, it will still be included (depending on the topological relationship used) but in its entirety (i.e. no trimming!). As we’re using point data, it is generally easier to use a subset approach. There are multiple ways to conduct spatial subsetting within R: First, we can either use [] just like you would use for selecting and slicing a normal (table-based) dataframe from R’s base package - or we can use the filter() function from dplyr within the tidyverse. Second, sf has its own library of subsetting through geometric operations, including: intersection, difference, symmetrical difference and snap. To keep things simple, we will use the base subsetting approach - which also works similarly when programming in Python, for instance. # subset London schools london_schools_prj_ss &lt;- london_schools_prj[london_outline,] Note In a case like above, you can just overwrite the current london_schools_prj variable as you know it is the dataset you want to use. Much of this code could be condensed into several lines using pipes to make our code shorter and more efficient - but then it would be harder to explain! As you progress with R and programming, you are welcome to bring pipes and restructuring into own your code - but even if you don’t, as long as your code does what you need it to do, then that’s our main aim with this course! Once you have run the above code, you should notice that your london_schools_prj_ss variable now only contains 3,372 records, instead of the original 3,889. We can also plot our variable using the same code as above, to double-check that it worked: # inspect tm_shape(london_wards) + tm_polygons() + tm_shape(london_schools_prj_ss) + tm_dots() We should now see that our schools are all contained within our ward dataset, so we know this dataset is ready to be used for analysis. We will now explore which schools are within 400m of greenspace and which are not. But first, we need to get our greenspace data ready so we can create the 400m buffers needed for this analysis. 1.6.4 Unioning We’ve done a lot of processing so far to do with our schools and ward data, but now it’s time for the greenspace datasets. If you look back at your code, you should remember that we have two datasets for our greenspace in London, which we now need to join together. This type of join is typically known as a union - and this is the type of tool you would want to look for across any GUI system. When it comes to programming, however, in either R or python, there is a much simpler way of joining datasets - and that’s simply copying over the records or observations from one variable into another - and the base library has a ready-to-go function for us to use, known as rbind(). This function allows you to ‘bind’ rows from one or more datasets together. This also works for sf objects. # join greenspace datasets together greenspace = rbind(TQ_greenspace, TL_greenspace) 1.6.5 Clipping The next step is to clip our reduced greenspace data to our London outline. Within sf, the clip function is known as the st_intersection() function - not to be confused with st_intersects() from above! A clip will change the geometry of some of our greenspaces on the outskirts of London, i.e. cookie-cut them precisely to the London outline. If we used the subset approach approach as we did earlier with our point data, we would simply extract all greenspaces that intersect with the London outline - but not change their geometry. What we can do however if reduce the processing required by our computer by using a mixture of these two methods - if we first subset our all_greenspace dataset by our London outline and then run the clip, our processing will be much faster: # subset and clip london_greenspace &lt;- greenspace[london_outline,] %&gt;% st_intersection(london_outline) ## Warning: attribute variables are assumed to be spatially constant throughout all ## geometries # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(london_greenspace) + tm_polygons() 1.6.6 Attribute selection Now we have only London greenspaces in our dataset, the next step, is to reduce the number of greenspaces to only those bigger than 50,000 square meters. To do this, we will use another type of subsetting you’ve probably come across, which is attribute subsetting - by using a simple query to subset only records that have an area larger than 50,000 square metres. To do this, we’ll use the filter() function from the dplyr library we mentioned earlier as well as another function called set_units() which is from the unit library that you’ve loaded - but we haven’t yet discussed. The set_units() function allows us to assign units to numerical values we are using within our query, i.e. here, for our query to run, our value must be in square metres to match the unit of the area_m column. To be able to query on our area, we must first calculate the area of each of our greenspaces. To do so in R, we can use the st_area() function within sf, which will calculate the area of each of our records/observations in our greenspace dataset. To store the output of this function as a new column in our london_greenspace dataset, we use a simple notation at the end of our london_greenspace variable: $area_m. The $ in R means for this data frame, access the column that proceeds this sign. In our case, we do not as yet have a column called area_m, therefore R will automatically create this column and then store the outputs of the function in this column: # calculate area london_greenspace$area_m &lt;- st_area(london_greenspace) Once we have our area column, we can now filter our dataset based on that column and filter out all greenspace with an area that is smaller than 50,000 square meters. # filter large greenspaces large_london_greenspace &lt;- london_greenspace %&gt;% filter(area_m &gt; set_units(50000.0, m^2)) We now can look at our final greenspace dataset against our london outline to see its final coverage: # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(large_london_greenspace) + tm_polygons() 1.6.7 Buffering We now have our London greenspace dataset - we are ready for the last step of processing with this dataset - generating our buffers that we can use to find all schools within 400 meters of the large greenspace areas. Once again, the sf library has a function for generating buffers - we just need to know how to deploy it successfully on our London greenspace dataset - and this involves understanding how to denote our distance correctly - as well as understanding if and how we can dissolve our buffer into a single record. To do this, we would investigate the documentation of the function st_buffer() to find out what additional parameters it takes - and how. What we can find out is that we need to (of course!) provide a distance for our buffer - but whatever figure we supply, this will be interpreted within the units of the CRS we are using. In our case, we are using British National Grid and, luckily for us, the units of the CRS is metres - which makes are life significantly easier when calculating these buffers. For other CRS, many use a base unit of an Arc Degree, e.g. WGS84. In this case, you technically have two options: 1) reproject your data into a CRS that uses metres as its base unit OR 2) convert your distance into an Arc Degree measurement. Always choose Option 1. Fortunately none of this is our concern - we know we can simply input the figure of 400 into our buffer and this will generate a buffer of 400m. # greenspace buffer gs_buffer_400m &lt;- st_buffer(large_london_greenspace, dist=400) As our final bit of processing with our greenspace buffer, we want to dissolve the whole buffer into a single record. To do this, we’ll replicate the code used for our London ward dissolve, creating a an area value for our buffer records in the process to be used within the summarisation - and then result in a new gs_buffer_400m_single variable: # dissolve greenspace buffer gs_buffer_400m_single &lt;- gs_buffer_400m %&gt;% summarise(area = sum(st_area(gs_buffer_400m))) # inspect tm_shape(london_outline) + tm_polygons() + tm_shape(gs_buffer_400m_single) + tm_polygons() 1.7 Greenspace in London Great, we are now ready to bring our two datasets together reaady for anlaysis - and to do so, we’ll use subsetting as well as the st_intersects() function, although with this one, we’ll use it in two different ways! Our first task is to identify those schools that have access to greenspace - and extract them to create a new variable for use within our final point-in-polygon count (i.e. how many schools within each ward has access to greenspace). As we know, we can subset our london_schools dataset by our greenspace buffer quite easily using the subset approach: # schools within 400m of greenspace london_schools_gs &lt;- london_schools_prj_ss[gs_buffer_400m_single,] Our london_schools_gs variable has been subsetted correctly if we end up with 1,770 records, instead of the 3,372 records we had previously. We can now use this dataset and our previous london_schools_prj_ss dataset to create counts at the ward level. But before we do that, we will create a binary attribute of greenspace access within our london_schools_prj_ss variable to visualise our school ‘points’. To do this, we’ll use the st_intersects() function mentioned above and add a new column, gs_access (i.e. greenspace access), which will tell us which schools have access to greenspace or not. The st_intersects() function is really useful as its output is a simple TRUE or FALSE statement - does this record intersect with the greenspace buffer? This result is what will be stored in our new column as a TRUE or FALSE response and what we can use to map our schools and their greenspace access: # greenspace access london_schools_prj_ss$gs_access &lt;- st_intersects(london_schools_prj_ss, gs_buffer_400m_single, sparse=FALSE) We could go ahead and recode this to create a 1 or 0, or YES or NO after processing, but for now we’ll leave it as TRUE or FALSE. We can go head and now visualise our schools based on this column, to see if they have access (TRUE) or do not have access (FALSE) to greenspace. To do this, we’ll use the tmap library again: # inspect tm_shape(london_schools_prj_ss) + tm_dots(col=&#39;gs_access&#39;, palette=&#39;BuGn&#39;) You’ll be pleased to read that we are finally here - we are at the last stage of our processing and can finally create the ward-level percentage of schools that have greenspace access, versus those that do not! To do this, we’ll be counting the number of points in each of our polygons, i.e. the number of schools in each ward. To do so in R and with sf, it is one line of code - which at first look does not sound at all like it is completing a point-in-polygon calculation - but it does! To create a PIP count within sf, we use the st_intersects() function again - but instead of using the output of TRUE or FALSE, what we actually extract from our function is its lengths recording. The lengths part of this function records how many times a join feature (i.e. our schools) intersects with our main features (i.e. our wards). (Note here, we do not set the sparse function to FALSE but leave it as TRUE/its default by not entering the parameter). As a result, the length of this list is equal to the count of how many schools are within the polygon - i.e. a PIP calculation. This is a really simple way of doing a PIP calculation - and makes it easy for us to store the output of the function and its lengths (and thus the count) directly as a column within our london_wards dataset, as so: # total number of schools in each ward london_wards$total_schools &lt;- lengths(st_intersects(london_wards, london_schools_prj_ss)) # total number of schools with greenspace access in each ward london_wards$gs_schools &lt;- lengths(st_intersects(london_wards, london_schools_gs)) As you can see from the code above, we’ve now calculated this for our total schools dataset and the schools that have access to greenspace. The final step in our processing therefore is to create our rate. To do so, we’ll use the same approach of generating a new column within our london_wards dataset - and then use a mathematical formula to calculate our rates: # percentage of schools with greenspace access london_wards$gs_rate &lt;- (london_wards$gs_schools/london_wards$total_schools)*100 And that’s it! We now have our greenspace rate for our wards, which we can now again map: # inspect tm_shape(london_wards) + tm_polygons(col=&#39;gs_rate&#39;, palette=&#39;Greens&#39;) We now have our final dataset ready for analysis. Right now, we haven’t introduced you to any statistical or spatial analysis techniques to fully analyse our dataset - but instead, we can focus on what are data shows visually! The last step of any programming is to extract our variables into permanent datasets for use at a later time. You can at any point in this practical, extract a permanent data file for each of our variables. For now, we’ll extract our new london_wards dataset as we might want to use this in some additional analysis that we could look at next week or for our assessments at a later stage. The great thing about coding this up now, is that it will be easy to re-run all of this analysis and export any of the variables, again, at a later time! # write st_write(obj=london_wards, dsn=&#39;data/london_ward_gs.shp&#39;, delete_dsn=TRUE) You should now see the dataset appear in your files! 1.8 Attributions This week’s content and practical uses content and inspiration from: Wilkin, Jo. 2020. Analysing school access to greenspace in London. https://github.com/jo-wilkin/R-GIS-Tutorials/blob/master/greenspace_access.Rmd "],["spatial-autocorrelation.html", "2 Spatial autocorrelation 2.1 This week 2.2 Case study 2.3 Neighbours 2.4 Getting started 2.5 Loading our datasets 2.6 Euclidean neighbours 2.7 Shared boundary neighbours 2.8 Theft in Camden 2.9 Global Moran’s I 2.10 Local Moran’s I 2.11 Attributions", " 2 Spatial autocorrelation 2.1 This week This week, we focus on the first of two key properties of spatial data: spatial dependence. Spatial dependence is the idea that the observed value of a variable in one location is often dependent (to some degree) on the observed value of the same value in a nearby location. For spatial analysis, this dependence can be assessed and measured statistically by considering the level of spatial autocorrelation between values of a specific variable, observed in either different locations or between pairs of variables observed at the same location. Spatial autocorrelation occurs when these values are not independent of one another and instead cluster together across geographic space. A critical first step of spatial autocorrelation is to define the criteria under which a spatial unit (e.g. an areal or point unit) can be understood as a “neighbor” to another unit. As highlighted in previous weeks, spatial properties can often take on several meanings, and as a result, have an impact on the validity and accuracy of spatial analysis. This multiplicity also can be applied to the concept of spatial neighbours which can be defined through adjacency, contiguity or distance-based measures. As the specification of these criteria can impact the results, the definition followed therefore need to be grounded in particular theory that aims to represent the process and variable investigated. 2.2 Case study This week looks at spatial dependence and autocorrelation in detail, focusing on the different methods of assessment. As part of this, we look at the multiple methods to defining spatial neighbours and their suitability of use across different spatial phenomena – and how this approach is used to generate spatial weights for use within these spatial autocorrelation methods as well as their potential to generate spatially-explicit variables. We put these learnings into practice through an analysis of spatial dependence of areal crime data, experimenting with the deployment of different neighbours and the impact of their analyses. For this practical we will look at the distribution of thefts from persons in the borough of Camden. 2.3 Neighbours If we want to come up with quantifiable descriptions of variables and how they vary over space, then we need to find ways of quantifying the distance from point to point. When you attach values to the polygons of wards in London, and visualise them, different patterns appear, and the different shapes and sizes of the polygons effect what these patterns look like. There can appear to be clusters, or the distribution can be random. If you want to explain and discuss variables, the underlying causes, and the possible solutions to issues, it becomes useful to quantify how clustered, or at the opposite end, how random these distributions are. This issue is known as spatial autocorrelation. In raster data, variables are measured at regular spatial intervals (or interpolated to be represented as such). Each measurement is regularly spaced from its neighbours, like the pixels on the screen you are reading this from. With vector data, the distance of measurement to measurement, and the size and shape of the “pixel” of measurement becomes part of the captured information. Whilst this can allow for more nuanced representations of spatial phenomena, it also means that the quantity and type of distance between measurements needs to be acknowledged. If you want to calculate the relative spatial relation of distributions, knowledge of what counts as a “neighbour” becomes useful. Neighbours can be neighbours due to euclidean distance (distance in space), or they can be due to shared relationships, like a shared boundary, or they can simply be the nearest neighbour, if there aren’t many other vectors around. Depending on the variable you are measuring the appropriateness of neighbourhood calculation techniques can change. 2.4 Getting started To begin let’s load the libraries needed for this practical. Install the libraries where necessary. # libraries library(sf) library(nngeo) library(data.table) library(tmap) library(tidyverse) Download the data necessary for the practical and put it in a folder we can find. We will use the same structure as the data directory from the previous week. If you are using the same directory for this weeks work then you can put these files in the same directories. If not make new ones with names that work. Alternatively you can make your own structure and use that. Note To follow along with the code as it is and not edit the file paths there should be a folder called data/ in your project working directory, if not best to make one. Inside this create a folder called crime. Put the 2019_camden_theft_from_person.csv file in that folder. Inside your data/ directory there should already be a folder called administrative_boundaries. Save the files belonging to the OAs_camden_2011.shp in here. File download File Type Link Theft in Camden shp, csv Download 2.5 Loading our datasets Now we have the data in the correct folders, we can load and plot the shape data. # load camden boundaries camden_oas &lt;- st_read(&#39;data/administrative_boundaries/OAs_camden_2011.shp&#39;, crs=27700) ## Reading layer `OAs_camden_2011&#39; from data source ## `/Users/Tycho/Dropbox/UCL/Web/jtvandijk.github.io/GEOG0114/data/administrative_boundaries/OAs_camden_2011.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 749 features and 17 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 523954.5 ymin: 180965.4 xmax: 531554.9 ymax: 187603.6 ## Projected CRS: OSGB 1936 / British National Grid # inspect tm_shape(camden_oas) + tm_polygons() You can see how one of these output areas could have many more neighbours than others, they vary a great deal in size and shape. The dimensions of these objects change over space, as a result the measurements within them must change too. Output areas are designed to convey and contain census information, so they are created in a way that maintains a similar number of residents in each one. The more sparsely populated an OA the larger it is. Output Areas are designed to cover the entirety of the land of England and Wales so they stretch over places where there are no people. In the north of Camden the largest Ouput Areas span over Hampstead Heath, a large park. Let’s explore how to find different kinds of neighbours using the example of one ‘randomly’ selected output area (E00004174) that happens to contain the UCL main campus. # highlight E00004174 tm_shape(camden_oas) + tm_borders(col=&#39;black&#39;) + tm_shape(camden_oas[camden_oas$OA11CD==&#39;E00004174&#39;,]) + tm_fill(col=&#39;red&#39;) 2.6 Euclidean neighbours The first way we are going to call something a neighbour is by using Euclidean distance. As our OA shapefile is projected in BNG (British National Grid), the coordinates are planar, going up 1 is the same distance as going sideways 1. Even better the coordinates are in metric measurements so it’s easy to make up heuristic distances. Let’s call every output area with a centroid 500m or less away from the centroid of our chosen OA a neighbour: we select only the the centroid of our chosen output area and all other areas (with st_centroid()) we set the maximum number of neighbours we want to find to “50” (with parameter k) we set the maximum distance of calling an OA centroid a neigbour to “500” (with parameter maxdist) we return a sparse matrix that tells us whether each OA is a neighbour or not (with parameter sparse) # assign our chosen OA to a variable chosen_oa &lt;- &#39;E00004174&#39; # identify neighbours chosen_oa_neighbours &lt;- st_nn(st_geometry(st_centroid(camden_oas[camden_oas$OA11CD==chosen_oa,])), st_geometry(st_centroid(camden_oas)), sparse = TRUE, k = 50, maxdist = 500) ## projected points # inspect class(chosen_oa_neighbours) ## [1] &quot;list&quot; # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # inspect tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &#39;green&#39;) + # highlight only the chosen OA tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + tm_fill(col = &#39;red&#39;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&#39;black&#39;) 2.7 Shared boundary neighbours The next way of calculating neighbours takes into account the actual shape and location of the polygons in our shapefile. This has only recently been added to the world of sf(), previously we would have reverted to using the sp() package and others that depend on it such as spdep(). We can create two functions that check whether any polygons share boundaries or overlap one another, and then also check by how much. These new functions are based on the st_relate() function. The different cases of these are known as queen, and rook. These describe the relations in a similar way to the possible chess board movements of these pieces. # for rook case st_rook = function(a, b = a) st_relate(a, b, pattern = &#39;F***1****&#39;) # for queen case st_queen &lt;- function(a, b = a) st_relate(a, b, pattern = &#39;F***T****&#39;) Now that we’ve created the functions lets try them out. # identify neighbours chosen_oa_neighbours &lt;- st_rook(st_geometry(camden_oas[camden_oas$OA11CD==chosen_oa,]), st_geometry(camden_oas)) # get the names (codes) of these neighbours neighbour_names &lt;- camden_oas[chosen_oa_neighbours[[1]],] neighbour_names &lt;- neighbour_names$OA11CD # inspect tm_shape(camden_oas) + tm_borders() + # highlight only the neighbours tm_shape(camden_oas[camden_oas$OA11CD %in% neighbour_names,]) + tm_fill(col = &#39;green&#39;) + tm_shape(camden_oas[camden_oas$OA11CD==chosen_oa,]) + # highlight only the chosen OA tm_fill(col = &#39;red&#39;) + tm_shape(camden_oas) + # overlay the borders tm_borders(col=&#39;black&#39;) Note Because the tolerance of the shared boundaries in the st_rook() pattern and the st_queen() pattern, in this example they both assign the same neighbours. This is true for many non-square polygons as the difference is often given as whether two shapes share one or more points. Therefore the difference can have more to do with the resolution and alignment of your polygons than the actual spatial properties they represent. They can and do find different neighbours in other situations. Follow the grid example in the st_relate() documentation if you want to see it working. 2.8 Theft in Camden Now that we have found the different ways of finding neighbours we can consider how they relate to one another. There are two ways of looking at spatial autocorrelation: Global: This is a way of creating a metric of how regularly or irregularly clustered the variables are over the entire area studied. Local: This is the difference between an area and its neighbours. You would expect neighbours to be similar, but you can find exceptional places and results by seeing if places are quantifiably more like or dislike their neighbours than the average other place. But before we start that let’s get into the data we are going to use! We’ll be using personal theft data from around Camden. Our neighbourhood analysis of spatial autocorrelation should allow us to quantify the pattern of distribution of reported theft from persons in Camden in 2019. # load theft data camden_theft &lt;- fread(&#39;data/crime/2019_camden_theft_from_person.csv&#39;) # convert csv to sf object camden_theft &lt;- st_as_sf(camden_theft, coords = c(&#39;X&#39;,&#39;Y&#39;), crs = 27700) # inspect tm_shape(camden_oas) + tm_polygons() + tm_shape(camden_theft) + tm_dots() This is point data, but we are interested in the polygons and how this data relates to the administrative boundaries it is within. Let’s count the number of thefts in each OA. This is a spatial operation that is often called “point in polygon”. As we are just counting the number of occurrences in each polygon it is quite easy. In the future you may often want to aggregate over points for an area, or in reverse assign values from the polygon to the points. # thefts in camden camden_oas$n_thefts &lt;- lengths(st_intersects(camden_oas, camden_theft)) # inspect tm_shape(camden_oas) + tm_fill(col=&#39;n_thefts&#39;) You can see our map isskewed by central London, meaning that the results in central London (the south of Camden) are so much larger than those in the north that it makes it harder to see the smaller differences between other areas. We’ll take the square root of the number of thefts to remedy this. # square root of thefts camden_oas$sqrt_n_thefts &lt;- sqrt(camden_oas$n_thefts) # inspect tm_shape(camden_oas) + tm_fill(col=&#39;sqrt_n_thefts&#39;) There: a slightly more nuanced picture 2.9 Global Moran’s I With a Global Moran’s I we can test how “random” the spatial distribution of these values is. Global Moran’s I is a metric between -1 and 1. -1 is a completely even spatial distribution of values, 0 is a “random” distribution, and 1 is a “non-random” distribution of clearly defined clusters. To calculate the Global Moran’s I you need an adjacency matrix that contains the information of whether or not an OA is next to another. For an even more nuanced view you can include distance, or a distance weighting in the matrix rather than just the TRUE or FALSE, to take into account the strength of the neighbourhoodness. Because of the way Moran’s I functions in R it is necessary to use the sp and spdep libraries. So load them in. # libraries library(sp) library(spdep) ## Loading required package: spData ## To access larger datasets in this package, install the spDataLarge ## package with: `install.packages(&#39;spDataLarge&#39;, ## repos=&#39;https://nowosad.github.io/drat/&#39;, type=&#39;source&#39;)` As you will see these methods and functions have quite esoteric and complicated syntax. Some of the operations they will do will be similar to the examples shown earlier, but the way they assign and store variables makes it much quicker to run complex spatial operations. # inspect class(camden_oas) ## [1] &quot;sf&quot; &quot;data.frame&quot; # convert to sp camden_oas_sp &lt;- as_Spatial(camden_oas, IDs=camden_oas$OA11CD) # inspect class(camden_oas_sp) ## [1] &quot;SpatialPolygonsDataFrame&quot; ## attr(,&quot;package&quot;) ## [1] &quot;sp&quot; Now we can make the esoteric and timesaving “nb” object in which we store for each OA which other OAs are considered to be neighbours. # create an nb object camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # inspect class(camden_oas_nb) ## [1] &quot;nb&quot; # inspect str(camden_oas_nb,list.len=10) ## List of 749 ## $ : int [1:7] 10 15 215 303 327 375 464 ## $ : int [1:5] 19 72 309 365 430 ## $ : int [1:3] 133 152 709 ## $ : int [1:7] 78 131 152 286 314 582 651 ## $ : int [1:5] 67 316 486 492 703 ## $ : int [1:8] 7 68 317 487 556 612 625 638 ## $ : int [1:3] 6 68 317 ## $ : int [1:7] 57 58 164 358 429 605 684 ## $ : int [1:5] 58 164 489 609 700 ## $ : int [1:7] 1 215 245 311 327 366 644 ## [list output truncated] ## - attr(*, &quot;class&quot;)= chr &quot;nb&quot; ## - attr(*, &quot;region.id&quot;)= chr [1:749] &quot;E00004395&quot; &quot;E00004314&quot; &quot;E00004578&quot; &quot;E00004579&quot; ... ## - attr(*, &quot;call&quot;)= language poly2nb(pl = camden_oas_sp, row.names = camden_oas_sp$OA11CD) ## - attr(*, &quot;type&quot;)= chr &quot;queen&quot; ## - attr(*, &quot;sym&quot;)= logi TRUE Next, we need to assign weights to each neighboring polygon. In our case, each neighboring polygon will be assigned equal weight with style='W'. After this, we can calculate a value for the Global Moran’s I. # create the list weights object nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;W&#39;) # inspect class(nb_weights_list) ## [1] &quot;listw&quot; &quot;nb&quot; # Moran&#39;s I mi_value &lt;- moran(camden_oas_sp$n_thefts,nb_weights_list,n=length(nb_weights_list$neighbours),S0=Szero(nb_weights_list)) # inspect mi_value ## $I ## [1] 0.4772137 ## ## $K ## [1] 75.21583 The Global Moran’s I seems to indicate that there is indeed some spatial autocorrelation in our data, however, this is just a quick way to check the score. To do so properly we need to compare our score a randomly distributed version of the variables. We can do this by using something called a Monte Carlo simulation. # run a Monte Carlo simulation 599 times mc_model &lt;- moran.mc(camden_oas_sp$n_thefts, nb_weights_list, nsim=599) # inspect mc_model ## ## Monte-Carlo simulation of Moran I ## ## data: camden_oas_sp$n_thefts ## weights: nb_weights_list ## number of simulations + 1: 600 ## ## statistic = 0.47721, observed rank = 600, p-value = 0.001667 ## alternative hypothesis: greater This model shows that our distribution of thefts differs significantly from a random distribution. As such, we can conclude that there is significant spatial autocorrelation in our theft dataset. 2.10 Local Moran’s I With a measurement of local spatial autocorrelation we could find hotspots of theft that are surrounded by areas of much lower theft. According to the previous global statistic these are not randomly distributed pockets but would be outliers against the general trend of clusteredness! These could be areas that contain very specific locations, where interventions could be made that drastically reduce the rate of crime rather than other areas where there is a high level of ambient crime. # create an nb object camden_oas_nb &lt;- poly2nb(camden_oas_sp, row.names=camden_oas_sp$OA11CD) # create the list weights object nb_weights_list &lt;- nb2listw(camden_oas_nb, style=&#39;W&#39;) # Local Moran&#39;s I local_moran_camden_oa_theft &lt;- localmoran(camden_oas_sp$n_thefts, nb_weights_list) To properly utilise these local statistics and make an intuitively useful map, we need to combine them with our crime count variable. Because of the way the new variable will be calculated, we first need to rescale our variable so that the mean is 0. # rescale camden_oas_sp$scale_n_thefts &lt;- scale(camden_oas_sp$n_thefts) To compare this rescaled value against its neighbours, we subseuqnetly need to create a new column that carries information about the neighbours. This is called a spatial lag function. The “lag” just refers to the fact you are comparing one observation against another, this can also be used between timed observations. In this case, the “lag” we are looking at is between neighbours. # create a spatial lag variable camden_oas_sp$lag_scale_n_thefts &lt;- lag.listw(nb_weights_list, camden_oas_sp$scale_n_thefts) Now we have used sp for all it is worth it’s time to head back to the safety of sf() before exploring any forms of more localised patterns. # convert to sf camden_oas_moran_stats &lt;- st_as_sf(camden_oas_sp) To make a human readable version of the map we will generate some labels for our findings from the Local Moran’s I stats. This process calculates what the value of each polygon is compared to its neighbours and works out if they are similar or dissimilar and in which way, then gives them a text label to describe the relationship. # set a significance value sig_level &lt;- 0.1 # classification with significance value camden_oas_moran_stats$quad_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;high-high&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;low-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;high-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0 &amp; local_moran_camden_oa_theft[,5] &lt;= sig_level, &#39;low-high&#39;, ifelse(local_moran_camden_oa_theft[,5] &gt; sig_level, &#39;not-significant&#39;, &#39;not-significant&#39;))))) # classification without significance value camden_oas_moran_stats$quad_non_sig &lt;- ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &#39;high-high&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &#39;low-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &gt; 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &lt;= 0, &#39;high-low&#39;, ifelse(camden_oas_moran_stats$scale_n_thefts &lt;= 0 &amp; camden_oas_moran_stats$lag_scale_n_thefts &gt; 0, &#39;low-high&#39;,NA)))) To understand how this is working we can look at the data non-spatially. As we rescaled the data, our axes should split the data neatly into their different area vs spatial lag relationship categories. Let’s make the scatterplot using the scaled number of thefts for the areas in the x axis and their spatially lagged results in the y axis. # plot the results without the satistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_non_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&#39;Scaled Thefts (n)&#39;) + ylab(&#39;Lagged Scaled Thefts (n)&#39;) + labs(colour=&#39;Relative to neighbours&#39;) + geom_point() # plot the results nnw with the satistical significance ggplot(camden_oas_moran_stats, aes(x = scale_n_thefts, y = lag_scale_n_thefts, color = quad_sig)) + geom_vline(xintercept = 0) + # plot vertical line geom_hline(yintercept = 0) + # plot horizontal line xlab(&#39;Scaled Thefts (n)&#39;) + ylab(&#39;Lagged Scaled Thefts (n)&#39;) + labs(colour=&#39;Relative to neighbours&#39;) + geom_point() Now let’s see how they are arranged spatially. # map all of the results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &#39;quad_non_sig&#39;) # map only the statistically significant results here tm_shape(camden_oas_moran_stats) + tm_fill(col = &#39;quad_sig&#39;) As our data are so spatially clustered we can’t see any outlier places (once we have ignored the non-significant results). This suggests that the pattern of theft from persons is not highly concentrated in very small areas or particular Output Areas, and instead is spread on a larger scale than we have used here. To go further than we have today it would be possible to run the exact same code but using a larger scale, perhaps LSOA, or Ward, and compare how this changes the Moran’s I statistics globally and locally. Or, to gain statistical significance in looking at the difference between areas getting more data perhaps over a longer timescale, where there are less areas with 0 thefts. 2.11 Attributions This week’s content and practical uses content and inspiration from: Long, Alfie. 2020. Spatial autocorrelation. https://github.com/jtvandijk/GEOG0114_20202021/blob/master/04-week04.Rmd Gimond, Manual. 2021. Spatial autocorrelation in R. https://mgimond.github.io/Spatial/spatial-autocorrelation-in-r.html "],["point-pattern-analysis.html", "3 Point pattern analysis", " 3 Point pattern analysis "],["geodemographics.html", "4 Geodemographics", " 4 Geodemographics "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
